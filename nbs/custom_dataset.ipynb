{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829575eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Multi_alphabet_Text_Recognition.xml_parser import read_kaist_xml\n",
    "from Multi_alphabet_Text_Recognition.cropping import crop_text_from_image_korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bea9705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, targ_dir: str, dataset: str, transform=None) -> None:\n",
    "        if dataset == 'arab':\n",
    "            pass\n",
    "#             self.annotations = generate_dataset_instances_eng_arab(targ_dir)\n",
    "        elif dataset == 'korean':\n",
    "            self.annotations = generate_dataset_instances_korean(targ_dir)\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.default_transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        image_path = self.annotations[index][0]\n",
    "        return Image.open(image_path) \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        img = self.load_image(index)\n",
    "        transcript = self.annotations[index][1]\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(img), transcript\n",
    "        else:\n",
    "            return self.default_transform(img), transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def generate_dataset_instances_korean(path: Path) -> Tuple[str, str]:\n",
    "    return_tuple_list = []\n",
    "    index = 0\n",
    "    for image_name in Path(path).glob('*.jpg'):\n",
    "        xml_path = str(image_name)[0:-3] + 'xml'\n",
    "        words = read_kaist_xml(xml_path)\n",
    "        if words[1] == True:\n",
    "            for i in range(len(words[0])):\n",
    "                if words[0][i] == '':\n",
    "                    continue\n",
    "                image_save_path = '../../datasets/korean/KAIST_cropped/' + str(index) + '.jpg'\n",
    "                crop_text_from_image_korean(str(image_name), words[2][i][0], words[2][i][1], words[2][i][2], words[2][i][3], image_save_path)\n",
    "                index += 1\n",
    "                return_tuple_list.append((image_save_path, words[0][i]))\n",
    "            \n",
    "    return return_tuple_list\n",
    "\n",
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    "    \"\"\"Plots a series of random images from image_paths.\n",
    "\n",
    "    Will open n image paths from image_paths, transform them\n",
    "    with transform and plot them side by side.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of target image paths. \n",
    "        transform (PyTorch Transforms): Transforms to apply to images.\n",
    "        n (int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib \n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0) \n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n",
    "            \n",
    "        \n",
    "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
    "                          n: int = 2,\n",
    "                          display_shape: bool = True,\n",
    "                          seed: int = None):\n",
    "    \n",
    "    # 2. Adjust display if n too high\n",
    "    if n > 10:\n",
    "        n = 10\n",
    "        display_shape = False\n",
    "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
    "    \n",
    "    # 3. Set random seed\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # 4. Get random sample indexes\n",
    "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
    "\n",
    "    # 5. Setup plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # 6. Loop through samples and display random samples \n",
    "    for i, targ_sample in enumerate(random_samples_idx):\n",
    "        targ_image, targ_transcription = dataset[targ_sample][0], dataset[targ_sample][1]\n",
    "\n",
    "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
    "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
    "\n",
    "        # Plot adjusted samples\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(targ_image_adjust)\n",
    "        plt.axis(\"off\")\n",
    "        print(targ_transcription)\n",
    "        if display_shape:\n",
    "            title = f\"\\nshape: {targ_image_adjust.shape}\"\n",
    "            plt.title(title)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
